#!/usr/bin/env python3
import logging
import pickle
from pathlib import Path
from typing import Literal, Any
from collections.abc import Iterable
from dataclasses import dataclass
from dotenv import load_dotenv
import annoy
import os

from livekit.agents import (
    JobContext,
    WorkerOptions,
    cli,
    RunContext,
    RoomInputOptions,
    Agent,
    AgentSession,
    function_tool,
)
from livekit.plugins import deepgram, silero, cartesia, openai
from livekit.plugins.turn_detector.english import EnglishModel

# Load .env variables
load_dotenv(dotenv_path=Path(__file__).parent.parent / ".env")

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("rag-agent")

# ---- Annoy + Metadata ----

Metric = Literal["angular", "euclidean", "manhattan", "hamming", "dot"]
ANNOY_FILE = "index.annoy"
METADATA_FILE = "metadata.pkl"

@dataclass
class _FileData:
    f: int
    metric: Metric
    userdata: dict[int, Any]

@dataclass
class Item:
    i: int
    userdata: Any
    vector: list[float]

@dataclass
class QueryResult:
    userdata: Any
    distance: float

class AnnoyIndex:
    def __init__(self, index: annoy.AnnoyIndex, filedata: _FileData) -> None:
        self._index = index
        self._filedata = filedata

    @classmethod
    def load(cls, path: str) -> "AnnoyIndex":
        p = Path(path)
        index_path = p / ANNOY_FILE
        metadata_path = p / METADATA_FILE

        with open(metadata_path, "rb") as f:
            metadata: _FileData = pickle.load(f)

        index = annoy.AnnoyIndex(metadata.f, metadata.metric)
        index.load(str(index_path))
        return cls(index, metadata)

    def query(self, vector: list[float], n: int, search_k: int = -1) -> list[QueryResult]:
        ids = self._index.get_nns_by_vector(
            vector, n, search_k=search_k, include_distances=True
        )
        return [
            QueryResult(userdata=self._filedata.userdata[i], distance=distance)
            for i, distance in zip(*ids)
        ]

# ---- Agent ----

class RAGEnrichedAgent(Agent):
    def __init__(self) -> None:
        super().__init__(
            instructions="""
                You are a helpful voice assistant for LiveKit, able to answer user questions.
                Keep answers casual, TTS-friendly, and avoid long formalities or markdown.
            """
        )
        vdb_dir = Path(__file__).parent / "data"
        data_path = vdb_dir / "paragraphs.pkl"

        if not vdb_dir.exists() or not data_path.exists():
            logger.warning("RAG database not found. Please run build_rag_data.py first.")
            return

        self._index_path = vdb_dir
        self._data_path = data_path
        self._seen_results = set()

        try:
            self._annoy_index = AnnoyIndex.load(str(self._index_path))
            with open(self._data_path, "rb") as f:
                self._paragraphs_by_uuid = pickle.load(f)
            logger.info("RAG database loaded successfully.")
        except Exception as e:
            logger.error(f"Failed to load RAG database: {e}")

    @function_tool
    async def livekit_docs_search(self, context: RunContext, query: str):
        """Search knowledge base for LiveKit information using the given query."""
        try:
            # TEMP: Use dummy vector â€” replace with real embedding if available
            query_vector = [0.01] * 1536

            all_results = self._annoy_index.query(query_vector, n=5)
            new_results = [r for r in all_results if r.userdata not in self._seen_results]

            if not new_results:
                return "No new results found."
            new_results = new_results[:2]

            context_parts = []
            for result in new_results:
                self._seen_results.add(result.userdata)
                paragraph = self._paragraphs_by_uuid.get(result.userdata, "")
                if paragraph:
                    source = "Unknown source"
                    if "from [" in paragraph:
                        source = paragraph.split("from [")[1].split("]")[0]
                        paragraph = paragraph.split("]")[1].strip()
                    context_parts.append(f"Source: {source}\nContent: {paragraph}\n")

            if not context_parts:
                return "No usable paragraph found."

            return "\n\n".join(context_parts)

        except Exception as e:
            logger.error(f"Search failed: {e}")
            return "Could not find any relevant information for that query."

    async def on_enter(self):
        self.session.generate_reply(
            instructions="Briefly greet the user and offer help with LiveKit."
        )

# ---- Entrypoint ----

async def entrypoint(ctx: JobContext):
    await ctx.connect()

    session = AgentSession(
        stt=deepgram.STT(
            model="nova-2",
            language="en-US"
        ),
        llm=openai.LLM.with_ollama(
            model="llama3.2:1b",
            base_url="http://localhost:11434/v1",
            temperature=0.8
        ),
        tts=cartesia.TTS(
            model="sonic-english",
            voice="c2ac25f9-ecc4-4f56-9095-651354df60c0",
            speed=0.5,
            emotion=["curiosity:highest", "positivity:high"]
        ),
        vad=silero.VAD.load()
    )

    await session.start(
        agent=RAGEnrichedAgent(),
        room=ctx.room,
        room_input_options=RoomInputOptions()
    )

    await session.generate_reply(
        instructions="Welcome! You're now connected to a LiveKit voice assistant. Ask me anything about LiveKit!"
    )

# ---- Main ----

if __name__ == "__main__":
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint))
